[{"authors":["admin"],"categories":null,"content":"I am a second year Ph.D. student in Machine Learning at the Georgia Institute of Technology, working with Prof. Sebastian Pokutta. My work is currently aimed at designing novel convex optimization algorithms with solid theoretical convergence guarantees and good numerical performance.\nPrior to joining the Ph.D. program I worked at HP as an R\u0026amp;D Systems Engineer for two years. I obtained a bachelor of science in Industrial Engineering from the Universidad Politécnica de Madrid and a Master of Science in Applied Physics from Cornell University.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a second year Ph.D. student in Machine Learning at the Georgia Institute of Technology, working with Prof. Sebastian Pokutta. My work is currently aimed at designing novel convex optimization algorithms with solid theoretical convergence guarantees and good numerical performance.\nPrior to joining the Ph.D. program I worked at HP as an R\u0026amp;D Systems Engineer for two years. I obtained a bachelor of science in Industrial Engineering from the Universidad Politécnica de Madrid and a Master of Science in Applied Physics from Cornell University.","tags":null,"title":"Alejandro Carderera","type":"authors"},{"authors":["Alejandro Carderera","Sebastian Pokutta"],"categories":[],"content":"","date":1582224390,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582224390,"objectID":"976f2be20e3ba0621945f48898603d97","permalink":"/publication/socgs/","publishdate":"2020-02-20T13:46:30-05:00","relpermalink":"/publication/socgs/","section":"publication","summary":"Constrained second-order convex optimization algorithms are the method of choice when a high accuracy solution to a problem is needed, due to their local quadratic convergence. These algorithms require the solution of a constrained quadratic subproblem at every iteration. We present the Second-Order Conditional Gradient Sliding (SOCGS) algorithm, which uses a projection-free algorithm to solve the constrained quadratic subproblems inexactly. When the feasible region is a polytope the algorithm converges quadratically in primal gap after a finite number of linearly convergent iterations. Once in the quadratic regime the SOCGS algorithm requires $\\mathcal{O}(\\log(\\log 1/\\varepsilon))$ first-order and Hessian oracle calls and $\\mathcal{O}(\\log (1/\\varepsilon) \\log(\\log1/\\varepsilon))$ linear minimization oracle calls to achieve an $\\varepsilon$-optimal solution. This algorithm is useful when the feasible region can only be accessed efficiently through a linear optimization oracle, and computing first-order information of the function, although possible, is costly.","tags":[],"title":"Second-order Conditional Gradient Sliding - Preprint","type":"publication"},{"authors":[],"categories":null,"content":"","date":1576351800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576351800,"objectID":"7faaf7f63b6093c46b53522c9c8c6eb2","permalink":"/talk/mlopt2019/","publishdate":"2019-11-04T16:00:37-05:00","relpermalink":"/talk/mlopt2019/","section":"talk","summary":"","tags":[],"title":"Breaking the Curse of Dimensionality (Locally) to Accelerate Conditional Gradients","type":"talk"},{"authors":[],"categories":null,"content":"","date":1574274600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574274600,"objectID":"7748d09dd6ccae435c4121ef374504cc","permalink":"/talk/fields2019/","publishdate":"2019-11-04T15:53:25-05:00","relpermalink":"/talk/fields2019/","section":"talk","summary":"","tags":[],"title":"Locally Accelerated Conditional Gradients","type":"talk"},{"authors":[],"categories":null,"content":"","date":1564423200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564423200,"objectID":"392b826a75a5f104be48c909bf6e8ef6","permalink":"/talk/riken2019/","publishdate":"2019-11-04T15:53:25-05:00","relpermalink":"/talk/riken2019/","section":"talk","summary":"","tags":[],"title":"Locally Accelerated Conditional Gradients","type":"talk"},{"authors":["Jelena Diakonikolas","Alejandro Carderera","Sebastian Pokutta"],"categories":[],"content":"","date":1560969990,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560969990,"objectID":"a3c0869064243ada520a16710e2ba1be","permalink":"/publication/locally-accelerated-conditional-gradients/","publishdate":"2019-11-04T13:46:30-05:00","relpermalink":"/publication/locally-accelerated-conditional-gradients/","section":"publication","summary":"Conditional gradients constitute a class of projection-free first-order algorithms for smooth convex optimization. As such, they are frequently used in solving smooth convex optimization problems over polytopes, for which the computational cost of orthogonal projections would be prohibitive. However, they do not enjoy the optimal convergence rates achieved by projection-based accelerated methods; moreover, achieving such globally-accelerated rates is information-theoretically impossible for these methods. To address this issue, we present Locally Accelerated Conditional Gradients -- an algorithmic framework that couples accelerated steps with conditional gradient steps to achieve local acceleration on smooth strongly convex problems. Our approach does not require projections onto the feasible set, but only on (typically low-dimensional) simplices, thus keeping the computational cost of projections at bay. Further, it achieves the optimal accelerated local convergence. Our theoretical results are supported by numerical experiments, which demonstrate significant speedups of our framework over state of the art methods in both per-iteration progress and wall-clock time.","tags":[],"title":"Locally Accelerated Conditional Gradients - AISTATS 2020","type":"publication"}]