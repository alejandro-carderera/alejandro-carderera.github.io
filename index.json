[{"authors":["admin"],"categories":null,"content":"I am a fourth year Ph.D. student in Machine Learning at the Georgia Institute of Technology, working with Prof. Sebastian Pokutta. My work is currently aimed at designing novel convex optimization algorithms with solid theoretical convergence guarantees and good numerical performance.\nPrior to joining the Ph.D. program I worked at HP as an R\u0026amp;D Systems Engineer for two years. I obtained a bachelor of science in Industrial Engineering from the Universidad Politécnica de Madrid and a Master of Science in Applied Physics from Cornell University.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a fourth year Ph.D. student in Machine Learning at the Georgia Institute of Technology, working with Prof. Sebastian Pokutta. My work is currently aimed at designing novel convex optimization algorithms with solid theoretical convergence guarantees and good numerical performance.\nPrior to joining the Ph.D. program I worked at HP as an R\u0026amp;D Systems Engineer for two years. I obtained a bachelor of science in Industrial Engineering from the Universidad Politécnica de Madrid and a Master of Science in Applied Physics from Cornell University.","tags":null,"title":"Alejandro Carderera","type":"authors"},{"authors":["Alejandro Carderera","Mathieu Besançon","Sebastian Pokutta"],"categories":[],"content":"","date":1622227590,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622227590,"objectID":"c220b55e2a76bb46b48fb8356dc54346","permalink":"/publication/fw-for-gsc-functions/","publishdate":"2019-11-04T13:46:30-05:00","relpermalink":"/publication/fw-for-gsc-functions/","section":"publication","summary":"Generalized self-concordance is a key property present in the objective function of many important learning problems. We establish the convergence rate of a simple Frank-Wolfe variant that uses the open-loop step size strategy γt=2/(t+2), obtaining a O(1/t) convergence rate for this class of functions in terms of primal gap and Frank-Wolfe gap, where t is the iteration count. This avoids the use of second-order information or the need to estimate local smoothness parameters of previous work. We also show improved convergence rates for various common cases, e.g., when the feasible region under consideration is uniformly convex or polyhedral.","tags":[],"title":"Simple steps are all you need: Frank-Wolfe and generalized self-concordant functions - Under submission to NeurIPS'21","type":"publication"},{"authors":["Mathieu Besançon","Alejandro Carderera","Sebastian Pokutta"],"categories":[],"content":"","date":1618425990,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618425990,"objectID":"e0f3b227c9207aefada62557c5e9504d","permalink":"/publication/frankwolfe/","publishdate":"2019-11-04T13:46:30-05:00","relpermalink":"/publication/frankwolfe/","section":"publication","summary":"We present FrankWolfe.jl, an open-source implementation of several popular Frank-Wolfe and Conditional Gradients variants for first-order constrained optimization. The package is designed with flexibility and high-performance in mind, allowing for easy extension and relying on few assumptions regarding the user-provided functions. It supports Julia's unique multiple dispatch feature, and interfaces smoothly with generic linear optimization formulations using MathOptInterface.jl.","tags":[],"title":"FrankWolfe.jl - Under submission to the INFORMS Journal of Computing","type":"publication"},{"authors":["Alejandro Carderera","Jelena Diakonikolas","Cheuk Yin Lin","Sebastian Pokutta"],"categories":[],"content":"","date":1613155590,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613155590,"objectID":"673e69a3f40687970f80469dfa8d9ef7","permalink":"/publication/parameter-free-locally-accelerated-conditional-gradients/","publishdate":"2019-11-04T13:46:30-05:00","relpermalink":"/publication/parameter-free-locally-accelerated-conditional-gradients/","section":"publication","summary":"Projection-free conditional gradient (CG) methods are the algorithms of choice for constrained optimization setups in which projections are often computationally prohibitive but linear optimization over the constraint set remains computationally feasible. Unlike in projection-based methods, globally accelerated convergence rates are in general unattainable for CG. However, a very recent work on Locally accelerated CG (LaCG) has demonstrated that local acceleration for CG is possible for many settings of interest. The main downside of LaCG is that it requires knowledge of the smoothness and strong convexity parameters of the objective function. We remove this limitation by introducing a novel, Parameter-Free Locally accelerated CG (PF-LaCG) algorithm, for which we provide rigorous convergence guarantees. Our theoretical results are complemented by numerical experiments, which demonstrate local acceleration and showcase the practical improvements of PF-LaCG over non-accelerated algorithms, both in terms of iteration count and wall-clock time.","tags":[],"title":"Parameter-Free Locally Accelerated Conditional Gradients - ICML 2021","type":"publication"},{"authors":["Alejandro Carderera","Sebastian Pokutta","Christof Schütte","Martin Weiser"],"categories":[],"content":"","date":1610045190,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610045190,"objectID":"0eed2803e9158ac96a26218ac8004613","permalink":"/publication/cindy/","publishdate":"2019-11-04T13:46:30-05:00","relpermalink":"/publication/cindy/","section":"publication","summary":"Governing equations are essential to the study of nonlinear dynamics, often enabling the prediction of previously unseen behaviors as well as the inclusion into control strategies. The discovery of governing equations from data thus has the potential to transform data-rich fields where well-established dynamical models remain unknown. This work contributes to the recent trend in data-driven sparse identification of nonlinear dynamics of finding the best sparse fit to observational data in a large library of potential nonlinear models. We propose an efficient first-order Conditional Gradient algorithm for solving the underlying optimization problem. In comparison to the most prominent alternative algorithms, the new algorithm shows significantly improved performance on several essential issues like sparsity-induction, structure-preservation, noise robustness, and sample efficiency. We demonstrate these advantages on several dynamics from the field of synchronization, particle dynamics, and enzyme chemistry.","tags":[],"title":"CINDy: Conditional gradient-based Identification of Non-linear Dynamics - Preprint","type":"publication"},{"authors":[],"categories":null,"content":"","date":1605117600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605117600,"objectID":"c75418aa8c55abc4dfcb31399668edd0","permalink":"/talk/informs2020/","publishdate":"2020-08-26T15:53:25-05:00","relpermalink":"/talk/informs2020/","section":"talk","summary":"","tags":[],"title":"Projection-Free First Order Optimization","type":"talk"},{"authors":[],"categories":null,"content":"","date":1598464800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598464800,"objectID":"c1256669987e8133ebeb98c5e0d648a6","permalink":"/talk/aistats2020/","publishdate":"2020-08-26T15:53:25-05:00","relpermalink":"/talk/aistats2020/","section":"talk","summary":"","tags":[],"title":"Locally Accelerated Conditional Gradients","type":"talk"},{"authors":["Alejandro Carderera","Sebastian Pokutta"],"categories":[],"content":"","date":1582224390,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582224390,"objectID":"976f2be20e3ba0621945f48898603d97","permalink":"/publication/socgs/","publishdate":"2020-02-20T13:46:30-05:00","relpermalink":"/publication/socgs/","section":"publication","summary":"Constrained second-order convex optimization algorithms are the method of choice when a high accuracy solution to a problem is needed, due to their local quadratic convergence. These algorithms require the solution of a constrained quadratic subproblem at every iteration. We present the Second-Order Conditional Gradient Sliding (SOCGS) algorithm, which uses a projection-free algorithm to solve the constrained quadratic subproblems inexactly. When the feasible region is a polytope the algorithm converges quadratically in primal gap after a finite number of linearly convergent iterations. Once in the quadratic regime the SOCGS algorithm requires $\\mathcal{O}(\\log(\\log 1/\\varepsilon))$ first-order and Hessian oracle calls and $\\mathcal{O}(\\log (1/\\varepsilon) \\log(\\log1/\\varepsilon))$ linear minimization oracle calls to achieve an $\\varepsilon$-optimal solution. This algorithm is useful when the feasible region can only be accessed efficiently through a linear optimization oracle, and computing first-order information of the function, although possible, is costly.","tags":[],"title":"Second-order Conditional Gradient Sliding - Preprint","type":"publication"},{"authors":[],"categories":null,"content":"","date":1576351800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576351800,"objectID":"7faaf7f63b6093c46b53522c9c8c6eb2","permalink":"/talk/mlopt2019/","publishdate":"2019-11-04T16:00:37-05:00","relpermalink":"/talk/mlopt2019/","section":"talk","summary":"","tags":[],"title":"Breaking the Curse of Dimensionality (Locally) to Accelerate Conditional Gradients","type":"talk"},{"authors":[],"categories":null,"content":"","date":1574274600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574274600,"objectID":"7748d09dd6ccae435c4121ef374504cc","permalink":"/talk/fields2019/","publishdate":"2019-11-04T15:53:25-05:00","relpermalink":"/talk/fields2019/","section":"talk","summary":"","tags":[],"title":"Locally Accelerated Conditional Gradients","type":"talk"},{"authors":[],"categories":null,"content":"","date":1564423200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564423200,"objectID":"392b826a75a5f104be48c909bf6e8ef6","permalink":"/talk/riken2019/","publishdate":"2019-11-04T15:53:25-05:00","relpermalink":"/talk/riken2019/","section":"talk","summary":"","tags":[],"title":"Locally Accelerated Conditional Gradients","type":"talk"},{"authors":["Jelena Diakonikolas","Alejandro Carderera","Sebastian Pokutta"],"categories":[],"content":"","date":1560969990,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560969990,"objectID":"a3c0869064243ada520a16710e2ba1be","permalink":"/publication/locally-accelerated-conditional-gradients/","publishdate":"2019-11-04T13:46:30-05:00","relpermalink":"/publication/locally-accelerated-conditional-gradients/","section":"publication","summary":"Conditional gradients constitute a class of projection-free first-order algorithms for smooth convex optimization. As such, they are frequently used in solving smooth convex optimization problems over polytopes, for which the computational cost of orthogonal projections would be prohibitive. However, they do not enjoy the optimal convergence rates achieved by projection-based accelerated methods; moreover, achieving such globally-accelerated rates is information-theoretically impossible for these methods. To address this issue, we present Locally Accelerated Conditional Gradients -- an algorithmic framework that couples accelerated steps with conditional gradient steps to achieve local acceleration on smooth strongly convex problems. Our approach does not require projections onto the feasible set, but only on (typically low-dimensional) simplices, thus keeping the computational cost of projections at bay. Further, it achieves the optimal accelerated local convergence. Our theoretical results are supported by numerical experiments, which demonstrate significant speedups of our framework over state of the art methods in both per-iteration progress and wall-clock time.","tags":[],"title":"Locally Accelerated Conditional Gradients - AISTATS 2020","type":"publication"},{"authors":["Jelena Diakonikolas","Alejandro Carderera","Sebastian Pokutta"],"categories":[],"content":"","date":1558291590,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558291590,"objectID":"1060a9d8cc6dd320695f8c7c093989a6","permalink":"/publication/breaking-curse-dimensionality/","publishdate":"2019-05-19T13:46:30-05:00","relpermalink":"/publication/breaking-curse-dimensionality/","section":"publication","summary":"Conditional gradient methods constitute a class of first order algorithms for solving smooth convex optimization problems that are projection-free and numerically competitive. We present the Locally Accelerated Conditional Gradients algorithmic framework that relaxes the projection-freeness requirement to only require projection onto (typically low-dimensional) simplices and mixes accelerated steps with conditional gradient steps to achieve dimensionindependent local acceleration for smooth strongly convex functions. We prove that the introduced class of methods attains the asymptotically optimal convergence rate. Our theoretical results are supported by numerical experiments that demonstrate a speed-up both in wall-clock time and in per-iteration progress when compared to state-of-the-art conditional gradient variants.","tags":[],"title":"Breaking the Curse of Dimensionality (Locally) to Accelerate Conditional Gradients","type":"publication"}]